{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1scDR_yhCy6tcpMOtKvkJ8pUEfJt0_UCT","timestamp":1691320204942}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lNMH1gfF3cF","outputId":"1337e524-605f-4a59-ef4c-2c75efb54277","executionInfo":{"status":"ok","timestamp":1691316291383,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rudraksh _100","userId":"15220412555355844467"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qa_ONewhIVLB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d89bc9f7-6564-4bab-9f83-b9067da32137","executionInfo":{"status":"ok","timestamp":1691316291383,"user_tz":-330,"elapsed":3812,"user":{"displayName":"Rudraksh _100","userId":"15220412555355844467"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/MyDrive/test/TestASSIG\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd '/gdrive/MyDrive/test/TestASSIG'"]},{"cell_type":"code","source":["df = pd.read_excel('Input.xlsx')\n","for index, row in df.iterrows():\n","  url = row['URL']\n","  url_id = row['URL_ID']\n","\n","  # make Request\n","  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","  try:\n","    response = requests.get(url,headers=header)\n","  except:\n","    print(\"can't get response of {}\".format(url_id))\n","\n","  #Beautifulsoup\n","  try:\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","  except:\n","    print(\"can't get page of {}\".format(url_id))\n","  #title\n","  try:\n","    title = soup.find('h1').get_text()\n","  except:\n","    print(\"can't get title of {}\".format(url_id))\n","    continue\n","  #text\n","  article = \"\"\n","  try:\n","    for p in soup.find_all('p'):\n","      article += p.get_text()\n","  except:\n","    print(\"can't get text of {}\".format(url_id))\n","\n","  #create text file for each site\n","  file_name = '/gdrive/MyDrive/test/TestASSIG/TitleText/' + str(url_id) + '.txt'\n","  with open(file_name, 'w') as file:\n","    file.write(title + '\\n' + article)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXXMNkOohRgr","outputId":"cc59c8d8-d7c9-4b34-af25-6eeb8de55ef1","executionInfo":{"status":"ok","timestamp":1691316360614,"user_tz":-330,"elapsed":69233,"user":{"displayName":"Rudraksh _100","userId":"15220412555355844467"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["can't get title of 11668.0\n","can't get title of 17671.4\n"]}]},{"cell_type":"code","source":["\n","#directories\n","text_dir = \"/gdrive/MyDrive/test/TestASSIG/TitleText\"\n","stopwords_dir = \"/gdrive/MyDrive/test/TestASSIG/StopWords\"\n","sentment_dir = \"/gdrive/MyDrive/test/TestASSIG/MasterDictionary\"\n","\n","# get all the stopwords from its directory and store them in set\n","stop_words = set()\n","for files in os.listdir(stopwords_dir):\n","  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n","    stop_words.update(set(f.read().splitlines()))\n","\n","# load all text files and store them in docs\n","docs = []\n","for text_file in os.listdir(text_dir):\n","  with open(os.path.join(text_dir,text_file),'r') as f:\n","    text = f.read()\n","#tokenize given text file\n","    words = word_tokenize(text)\n","# remove  stop words from token\n","    filtered_text = [word for word in words if word.lower() not in stop_words]\n","# add filtered tokens of each file into a list\n","    docs.append(filtered_text)\n","\n","\n","\n","# store positive, negative words from sentiment directory\n","pos=set()\n","neg=set()\n","\n","for files in os.listdir(sentment_dir):\n","  if files =='positive-words.txt':\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      pos.update(f.read().splitlines())\n","  else:\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      neg.update(f.read().splitlines())\n","\n","# collect the positive  and negative words from each file\n","# calculate the score\n","positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","#iterate through docs\n","for i in range(len(docs)):\n","  positive_words.append([word for word in docs[i] if word.lower() in pos])\n","  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n","  positive_score.append(len(positive_words[i]))\n","  negative_score.append(len(Negative_words[i]))\n","  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n"],"metadata":{"id":"1tRdSv8ErMOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","avg_sentence_length = []\n","Percentage_of_Complex_words  =  []\n","Fog_Index = []\n","complex_word_count =  []\n","avg_syllable_word_count =[]\n","\n","# Percentage of Complex words = the number of complex words / the number of words\n","# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","stopwords = set(stopwords.words('english'))\n","def measure(file):\n","  with open(os.path.join(text_dir, file),'r') as f:\n","    text = f.read()\n","# remove punctuations\n","    text = re.sub(r'[^\\w\\s.]','',text)\n","# split the given text file into sentences\n","    sentences = text.split('.')\n","# total number of sentences in a file\n","    num_sentences = len(sentences)\n","# total no of words in the file\n","    words = [word  for word in text.split() if word.lower() not in stopwords ]\n","    num_words = len(words)\n","\n","    complex_words = []\n","    for word in words:\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word > 2:\n","        complex_words.append(word)\n","\n","# Syllable Count Per Word\n","#  words ending with 'es' and 'ed' not counted\n","    syllable_count = 0\n","    syllable_words =[]\n","    for word in words:\n","      if word.endswith('es'):\n","        word = word[:-2]\n","      elif word.endswith('ed'):\n","        word = word[:-2]\n","      vowels = 'aeiou'\n","      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","      if syllable_count_word >= 1:\n","        syllable_words.append(word)\n","        syllable_count += syllable_count_word\n","\n","\n","    avg_sentence_len = num_words / num_sentences\n","    avg_syllable_word_count = syllable_count / len(syllable_words)\n","    Percent_Complex_words  =  len(complex_words) / num_words\n","    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n","\n","    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n","\n","# iterate through every file\n","for file in os.listdir(text_dir):\n","  x,y,z,a,b = measure(file)\n","  avg_sentence_length.append(x)\n","  Percentage_of_Complex_words.append(y)\n","  Fog_Index.append(z)\n","  complex_word_count.append(a)\n","  avg_syllable_word_count.append(b)"],"metadata":{"id":"F8RaMuD_EnQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n","# We count the total cleaned words present in the text by\n","# removing the stop words (using stopwords class of nltk package).\n","# removing any punctuations like ? ! , . from the word before counting.\n","\n","def cleaned_words(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    text = re.sub(r'[^\\w\\s]', '' , text)\n","    words = [word  for word in text.split() if word.lower() not in stopwords]\n","    length = sum(len(word) for word in words)\n","    average_word_length = length / len(words)\n","  return len(words),average_word_length\n","\n","word_count = []\n","average_word_length = []\n","for file in os.listdir(text_dir):\n","  x, y = cleaned_words(file)\n","  word_count.append(x)\n","  average_word_length.append(y)\n","\n","\n","# To calculate Personal Pronouns mentioned in the text, we use regex to find\n","# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n","#  so that the country name US is not included in the list.\n","def count_personal_pronouns(file):\n","  with open(os.path.join(text_dir,file), 'r') as f:\n","    text = f.read()\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    count = 0\n","    for pronoun in personal_pronouns:\n","      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n","  return count\n","\n","pp_count = []\n","for file in os.listdir(text_dir):\n","  x = count_personal_pronouns(file)\n","  pp_count.append(x)"],"metadata":{"id":"4NElx7d94ICm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_df = pd.read_excel('Output Data Structure.xlsx')\n","\n","# URL_ID 11668,17671.4 don't exist\n","non_existent_url_ids = [11668,17671.4]\n","\n","output_df = output_df[~output_df['URL_ID'].isin(non_existent_url_ids)]\n","\n","# VARIABLES\n","variables = [positive_score,\n","            negative_score,\n","            polarity_score,\n","            subjectivity_score,\n","            avg_sentence_length,\n","            Percentage_of_Complex_words,\n","            Fog_Index,\n","            avg_sentence_length,\n","            complex_word_count,\n","            word_count,\n","            avg_syllable_word_count,\n","            pp_count,\n","            average_word_length]\n","\n","# load the files into the dataframe\n","for i, var in enumerate(variables):\n","  output_df.iloc[:,i+2] = var\n","\n","#save output to file\n","output_df.to_csv('Output_Data.csv')"],"metadata":{"id":"mXsnVluZ9TG3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691318746785,"user_tz":-330,"elapsed":470,"user":{"displayName":"Rudraksh _100","userId":"15220412555355844467"}},"outputId":"695bfe49-b273-4949-c215-0558e850d7f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-39a46952fa0e>:30: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n","  output_df.iloc[:,i+2] = var\n"]}]}]}